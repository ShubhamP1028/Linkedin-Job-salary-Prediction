# -*- coding: utf-8 -*-
"""skills_extraction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1BwF0YcCVNqLfTkyfm4VJCH3Y4RR1nqja

Linkedin Salary Prediction Model : skills extraction
"""

import pandas as pd
import re
import nltk
from nltk import word_tokenize
from collections import Counter

nltk.download('punkt_tab')

def clean_text(text):
    if pd.isna(text):
        return ''
    text = str(text).lower()
    text = re.sub(r'\*', '', text)
    text = re.sub(r'\s+', ' ', text).strip()
    return text

skills_list = [
    # Programming Languages & Libraries
    'python', 'r', 'sql', 'java', 'c++', 'pandas', 'numpy', 'scikit-learn', 'tensorflow',
    'pytorch', 'matplotlib', 'seaborn', 'jupyter', 'flask', 'django', 'spark', 'hadoop',
    'airflow', 'kafka', 'bigquery', 'redshift', 'snowflake',
    # Cloud & DevOps
    'aws', 'azure', 'gcp', 'terraform', 'cloudformation', 'docker', 'kubernetes', 'jenkins',
    'git', 'ansible', 'linux', 'helm', 'ci/cd', 'sagemaker', 'mlflow',
    # BI & Visualization
    'tableau', 'power bi', 'excel', 'ssrs', 'ssis', 'dax', 'dashboards',
    # ML/NLP
    'machine learning', 'deep learning', 'model deployment', 'feature engineering',
    'spaCy', 'nltk', 'transformers', 'bert', 'gpt', 'hugging face', 'text classification',
    'sentiment analysis', 'explainability', 'research',
    # Cybersecurity
    'penetration testing', 'firewalls', 'network security', 'ids', 'ips', 'siem',
    'kali linux', 'ethical hacking',
    'statistics', 'data pipelines', 'etl', 'storytelling', 'business analysis',
    'problem-solving', 'collaboration', 'data visualization', 'consulting', 'oopp', 'rest apis'
]

def extract_skills(text):
    tokens = word_tokenize(text)
    skills = set()
    for token in tokens:
        if token in skills_list:
            skills.add(token)
    text_lower = text.lower()
    if 'infrastructure as code' in text_lower or 'iac' in text_lower:
        skills.add('iac')
    if 'continuous integration' in text_lower or 'ci/cd' in text_lower:
        skills.add('ci/cd')
    if 'gitlab ci/cd' in text_lower:
        skills.add('gitlab')
    return list(skills)

def extract_requirements(text):
    requirements = []
    exp_match = re.search(r'experience:?\s*(\d+\+?\s*year[s]?)', text, re.IGNORECASE)
    if exp_match:
        requirements.append(exp_match.group(1))
    degree_match = re.search(r'(bachelor|master|phd|degree|certification)\s*(in)?\s*\w+', text, re.IGNORECASE)
    if degree_match:
        requirements.append(degree_match.group(0))
    return requirements

# Feature engineer: get the top skill by frequency of occurrence in text
def get_top_skill(skills, text):
    if not skills:
        return ''

    text = text.lower()
    skill_freq = {skill: text.count(skill.lower()) for skill in skills}
    top_skill = max(skill_freq, key=skill_freq.get)
    return top_skill

# Final processing function
def process_dataframe(df):
    df['description_cleaned'] = df['description'].apply(clean_text)
    df['skills'] = df['description_cleaned'].apply(extract_skills)
    df['requirements'] = df['description_cleaned'].apply(extract_requirements)
    df['top_skill'] = df.apply(lambda x: get_top_skill(x['skills'], x['description_cleaned']), axis=1)
    return df

